<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-04-20T15:11:44-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chanwoo’s notes</title><subtitle>NA
</subtitle><author><name>Chanwoo Chun</name><email>cc2465@cornell.edu</email></author><entry><title type="html">Abstraction</title><link href="http://localhost:4000/2023/04/17/Abstract-math-copy.html" rel="alternate" type="text/html" title="Abstraction" /><published>2023-04-17T00:00:00-04:00</published><updated>2023-04-17T00:00:00-04:00</updated><id>http://localhost:4000/2023/04/17/Abstract-math%20copy</id><content type="html" xml:base="http://localhost:4000/2023/04/17/Abstract-math-copy.html"><![CDATA[<h2 id="introduction-a-short-rant">Introduction (a short rant)</h2>
<p>I study machine learning and this blog, like many other PhD students’ blogs, covers topics in machine learning. No machine learning scientists need to know abstract algebra concepts in order to perform research. However, it is hard to read serious (theoretical) machine learning literature without understanding the terminologies used in them. This has been my rate-limiting step which I eager to improve.</p>

<p>This post is for you if you are also frustrated by phrases such as “$V$ is a vector space equipped with inner product” or scared whenever the word “tensor product” comes up, since you do not think you fully grasped what a tensor really is, and you only heard someone say “it is just a generalization of a matrix” (which is in fact kind of wrong; but it doesn’t matter in practice). I personally was puzzled by the “equipped with inner product” phrase, since it is like saying a Lego set comes with: 1. the bricks and 2. your ability to click these bricks together. If you give me bricks, I should be able to click them together no matter what. It doesn’t make sense my ability to click is attached to the Lego set. If you give me vectors, I should be able to take inner product no matter what. Why do they attach my ability to take inner product to the definition of certain vector spaces but not all vector spaces? I ask this question to people who are comfortable with abstract algebra, and they all reply “that is just the way it is”, and that makes it sound even more mysterious.</p>

<p>It all has to do with neuroplasticity of our math brain (whatever that subcircuit is in our cortex) and college curriculum. After taking Gilbert Strang’s “Linear Algebra”, your brain becomes an expert in handling array of numbers. If your brain is plastic enough, taking abstract algebra course will help you generalize the array of numbers to some abstractions. Unfortunately, if you never took abstract algebra and have thought of vectors as array of numbers for a decade, your brain has solidified around that notion. For our unfortunate souls, I aim to provide a sledge hammer to beak us free from that mind-jail and allow ourselves to read advanced papers like we eat breakfast. This is a blog post equipped with your ability to learn abtract math!</p>

<h2 id="goal-1-vector-space-equipped-with-inner-product">Goal 1: “Vector space equipped with inner product”</h2>
<p>Let’s understand what this means. We first need to free ourselves from vector = array of numbers mindset.</p>

<p>Vector space is comprised of two things: vectors drawn from a set $V$, and scalars drawn from a field $k$. We will define vector by showing how it interact with each other and scalars. However, we need to first know what “scalar” and “field” really means.</p>

<h3 id="field">Field</h3>

<p>I know you are thinking about $\mathbb{R}$ (real numbers) and $\mathbb{C}$ (complex numbers). Pretend like you don’t know that. But pretending you don’t know something you already know is very difficult. That is a skill that you need for abstraction.</p>

<p>Field is defined by axioms that dictates how its elements interact with each other. (modified from Dr. Victor’s note:)</p>

<ol>
  <li>
    <p>A field is a set of elements $\alpha, \beta, \ldots$ along with two operations, $\spadesuit$ and $\diamondsuit$.</p>
  </li>
  <li>
    <p>For the operation $\spadesuit$, the elements form a commutative group. The identity is denoted by $e_\spadesuit$. The inverse of $\alpha$ is denoted $\alpha_\spadesuit$.</p>
  </li>
  <li>
    <p>For the operation $\diamondsuit$, the elements other than $e_\spadesuit$ form a commutatitve group, and the identity is denoted by $e_\diamondsuit$. The inverse of $\alpha$ is denoted $\alpha_\diamondsuit$. (pg.12 error)</p>
  </li>
  <li>
    <p>The operations $\spadesuit$ and $\diamondsuit$ are linked by the distributive law, $\alpha \diamondsuit (\beta \spadesuit \gamma) = \alpha \diamondsuit \beta \spadesuit \alpha \diamondsuit \gamma$. But note that 
$\alpha \spadesuit (\beta \diamondsuit \gamma) \neq \alpha \spadesuit \beta \diamondsuit \alpha \spadesuit \gamma$.</p>
  </li>
  <li>
    <p>Fields can be finite or infinite</p>
  </li>
</ol>

<p>This may look completely arbitrary, but everything makes sense when you realize $\spadesuit$ is what is commonly known is $+$ and $\diamondsuit$ is $\cdot$. But resist that temptation for now.</p>

<p>It seems arbitrary that in 3., we are omitting $e_\spadesuit$. It is there because, apparently, $\alpha \diamondsuit e_\spadesuit = e_\spadesuit$. This means that $e_\spadesuit$ does not have inverse, since if it does exists (and denote it $e_{\spadesuit\diamondsuit}$) and assign it to $\alpha$, then we have $e_{\spadesuit\diamondsuit} \diamondsuit e_\spadesuit = e_\spadesuit$. However, the right hand side (RHS) should not be $e_\spadesuit$, but instead $e_\diamondsuit$. Now the question is, why did I assume that this is true: $\alpha \diamondsuit e_\spadesuit = e_\spadesuit$? Interestingly, this can be proved with the axioms provided above (for now, I refer to <a href="&quot;https://www.quora.com/How-we-can-prove-0-x-x-0-0-by-only-real-field-axioms&quot;">here</a> for the proof).</p>

<details>
    <summary>Questions that I have. Click to expand</summary>
    As of writing this, I am wondering how to show the order of operations given the field axioms... Also, how were these axioms constructed? It feels arbitrary, except for the fact that it perfectly explains the real numbers (and complex numberes) with addition and multiplication. Is it really useful to think of  it as something more general? What if we change the axioms and come up with different versions of field? I feel like this is a rabbit hole I should jump over... Ah the temptation to jump in though! But the time is finite.
</details>

<h3 id="vector-space">Vector space</h3>
<p>The idea of vector being an array of number and scalar being a real number is burned-in to my brain. I will use different names: omnis (Latin for “everything”, plural: omnes) and pondus (Latin for “weight”, plural: pondera).</p>

<p>Omnis space (vector space) have two kinds of mathematical objects: omnes ($o,p,\ldots$) drawn from a set $O$, and pondera ($\alpha,\beta,\ldots$) form a field $K$.</p>

<p>The field of pondus has operations $\spadesuit$ and $\diamondsuit$, the same notations used above.</p>

<p>The omnes form a communtative group under operation $\clubsuit$. The inverse of $o$ is $o_\clubsuit$.</p>

<p>There is a special operation called $\heartsuit: O\times K \rightarrow O$. If $o \in O$ and $\alpha \in K$, then $o \heartsuit \alpha \in O$. It obeys two kinds of distributive laws:</p>

<ol>
  <li>$\alpha \heartsuit (o \clubsuit p) = \alpha \heartsuit o \clubsuit \alpha \heartsuit p$</li>
  <li>$(\alpha \spadesuit \beta) \heartsuit o = \alpha \heartsuit o \clubsuit \beta \heartsuit o$</li>
</ol>

<p>Note that the RHS of 2. has $\clubsuit$ instead of $\spadesuit$!
Due to 2., $e_\spadesuit \heartsuit o$ is the identity for $\clubsuit$: $\alpha \heartsuit o = (\alpha \spadesuit e_\spadesuit)\heartsuit o = \alpha \heartsuit o \clubsuit e_\spadesuit \heartsuit o = (\alpha \heartsuit o) \clubsuit (e_\spadesuit \heartsuit o)$.
Now what is an inverse of $\alpha \heartsuit o$ under $\clubsuit$ (denoted <span>$( \alpha \heartsuit o)_\clubsuit $</span>)? <span>$e_\spadesuit \heartsuit o = (\alpha \spadesuit \alpha_\spadesuit) \heartsuit o = (\alpha \heartsuit o) \clubsuit (\alpha_\spadesuit \heartsuit o)$</span>. Hence <span>$( \alpha \heartsuit o)_\clubsuit $</span> <span>$=\alpha_\spadesuit \heartsuit o$.</span></p>

<p>There is also some sort of associative law that relates $\diamondsuit$ and $\heartsuit$, namely,  $(\alpha \diamondsuit \beta) \heartsuit o = \alpha \heartsuit (\beta \heartsuit o) $. Due to this, $e_\diamondsuit \heartsuit o = o$. In other words, $e_\diamondsuit$ ($=e_\heartsuit$) is an identity for $\heartsuit$. Why? $e_\diamondsuit \heartsuit ((e_\diamondsuit \heartsuit o) \clubsuit o_\clubsuit)  = (e_\diamondsuit \heartsuit (e_\diamondsuit \heartsuit o)) \clubsuit (e_\diamondsuit \heartsuit o_\clubsuit) = (e_\diamondsuit \heartsuit o) \clubsuit (e_\diamondsuit \heartsuit o_\clubsuit) = e_\diamondsuit\heartsuit(o \clubsuit o_\clubsuit)$
Now look at the left-most side and right-most side of the chain of equalities. The paranthesis that contains $e_\diamondsuit \heartsuit o$ in the left-most side must equal $o$ in the right-most side. Therefore, $e_\diamondsuit \heartsuit o = o$. But I haven’t shown that this equality is unique. Anyways, this is not that important for getting to our goal.</p>

<p>So these mathematical objects, omnis and pondus, are defined by their actions. We can further define what they are, by introducing additional actions. We will say that a new action introduces new quantity in the omnis space that satisfies the triangle inequality. We could call it “pressure” or “velocity”, or “momentum”, but “length” is the simplest concept that satisfies triangle inequality, so we will use “length”.</p>

<h2 id="appendix-group-theory">Appendix: Group theory</h2>

<h3 id="homomorphism">Homomorphism</h3>

<ol>
  <li>Onto homomorphism</li>
  <li>Isomorphism</li>
  <li>Automorphism</li>
</ol>

<h3 id="kernel">Kernel</h3>]]></content><author><name>Chanwoo Chun</name><email>cc2465@cornell.edu</email></author><summary type="html"><![CDATA[Introduction (a short rant) I study machine learning and this blog, like many other PhD students’ blogs, covers topics in machine learning. No machine learning scientists need to know abstract algebra concepts in order to perform research. However, it is hard to read serious (theoretical) machine learning literature without understanding the terminologies used in them. This has been my rate-limiting step which I eager to improve.]]></summary></entry><entry><title type="html">FAQ in abstract math and analysis</title><link href="http://localhost:4000/2023/04/17/Abstract-math.html" rel="alternate" type="text/html" title="FAQ in abstract math and analysis" /><published>2023-04-17T00:00:00-04:00</published><updated>2023-04-17T00:00:00-04:00</updated><id>http://localhost:4000/2023/04/17/Abstract-math</id><content type="html" xml:base="http://localhost:4000/2023/04/17/Abstract-math.html"><![CDATA[<p>This note is complementary to textbooks and lecture notes available online. I do not aim to be as complete as those materials in my coverage of each topic. Instead, I am writing this note as FAQ (maybe saying “frequent” is incorrect here since I am really just providing answers to questions that I had, which is of sample size 1.) There are some “zones” of abstract math that can be confusing for people without formal math training, that are not addressed in any textbook. Therefore, they can never escape that zone. Hopefully, this can be a ladder for their escape, since it has been for me.</p>

<p>To me, this note is a collection of concepts that were hard to understand, as I studied towards understadning vector-valued reproducing kernel Hilbert space. The papers on this topic use mathematical jargons that are effective but hard to understand without some training. I needed to familiarize myself with the concepts of tensor, tensor product, and Hilbert space, which are all steming from group theory and vector spaces.</p>

<h2 id="recommended-material">Recommended material</h2>
<p><a href="http://www-users.med.cornell.edu/~jdvicto/jdv/mathcourse2223/GFVS2223.pdf">This is a lecture note</a> by Jonathan D. Victor on group, fields, and vector spaces.</p>

<h2 id="group">Group</h2>

<p>Group refers to a set elements and an operation performed on the elements. The following “group axioms” needs to be satisfied for the set and the operation to be defined as a group. (Following is directly from Victor’s lecture note):</p>

<ol>
  <li>
    <p>Associativity: $a \circ (b \circ c) = (a \circ b) \circ c$</p>
  </li>
  <li>
    <p>Identity: There is a special element $e\in G$ ($G$ is a group) for which, for every $a$ in $G$, $a \circ e = a$ and $e \circ a = a$</p>
  </li>
  <li>
    <p>Existence of inverses: For every $a$ in $G$, there is a corresponding group element $a^{-1}$ for which $a \circ a^{-1} = e$ and $a^{-1} \circ a = e$. The operation need not be communtative.
The operation mapps a pair of elements to another element, i.e. $\circ: G\times G \rightarrow G$.
Group axioms define the fundamental and essential properties of a group, and they allow a group to be a set of transformations of an object that preserves some specified aspects of that object’s structure. This concretely and visually makes sense for a group such as dihedral group, but not so obvious even for a basic group such as $\mathbb{R}$ with addition. It turns out, the “object” in this case, is the real number line, and the “transformations” are translation along the number line (elements of $\mathbb{R}$ are the amount we translate along the line.) The object is preserved under the operation. In dihedral group, the object is a k-gon, and the transformations (or elements of the group) are rotations (for a square, 0, 90, 180, and 270 degree angle rotations) and reflections. The operation on the group is composition.</p>
  </li>
</ol>]]></content><author><name>Chanwoo Chun</name><email>cc2465@cornell.edu</email></author><summary type="html"><![CDATA[This note is complementary to textbooks and lecture notes available online. I do not aim to be as complete as those materials in my coverage of each topic. Instead, I am writing this note as FAQ (maybe saying “frequent” is incorrect here since I am really just providing answers to questions that I had, which is of sample size 1.) There are some “zones” of abstract math that can be confusing for people without formal math training, that are not addressed in any textbook. Therefore, they can never escape that zone. Hopefully, this can be a ladder for their escape, since it has been for me.]]></summary></entry><entry><title type="html">Tutorial on Neural Tangent Kernel</title><link href="http://localhost:4000/2023/04/14/NTK.html" rel="alternate" type="text/html" title="Tutorial on Neural Tangent Kernel" /><published>2023-04-14T14:35:00-04:00</published><updated>2023-04-14T14:35:00-04:00</updated><id>http://localhost:4000/2023/04/14/NTK</id><content type="html" xml:base="http://localhost:4000/2023/04/14/NTK.html"><![CDATA[<p>Despite the popularity and importance of Neural Tangent Kernel, the original paper by Jacot et al. is not an easy read. Thankfully, there are a lot of lecture notes and blog posts available online that explains NTK in very simple terms, but none (to the best of my knowledge) illustrates the generality of “tangent kernel” that explains the evolution of any parameteric model descending down on any kind of cost function. This is in part because “tangent kernel” part of the paper is a bit abstract. Moreover, the general public (myself included) is more interested in what the paper has to say about neural networks and least-squares cost, where the derivation is less abstract and easier to follow.</p>

<p>In my opinion, the tangent kernel part is impenetrable since they start with a list of seemingly arbitrary definitions of norms and kernels without providing motivations and contexts, and then show how they can be used. In this blog post, I start with the context and derivation that are skipped in the paper, from which it can be clearly understood why the authors had to come up with those definitions. Here the proofs are written in a way that no steps are skipped. This post is self-contained. None of the statements made here are my own contribution.</p>

<h1 id="construction-of-tangent-kernel">Construction of tangent kernel</h1>

<p>The result reviewed here is quite general and not limited to neural network models or least-squares cost function. The cost function is $C:\mathcal{F}\rightarrow\mathbb{R}$ and the model is $F:\mathbb{R}^{N}\rightarrow\mathcal{F}$ . An element of the function space $\mathcal{F}$ is $f:\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}$ . During training $f$ changes over time, so we may explicity show its time-dependency by writing $f(t)$. The output of the function for an input $x\in\mathbb{R}^{d}$ is explicity written as $f(t)(x)\in\mathbb{R}^{m}$. The cost function composed with the model is $C(f)$, or as $C(f(t))$ to show the time-dependency. Note that is $f(t)\in\mathcal{F}$, not $\in\mathbb{R}^{m}$.</p>

<p>We want to know how the cost value $C(f(t))$ evolves over time. For that, we take derivative of $C(f(t))$. Start with an incorrect chain rule:</p>

<p>\(\frac{\partial C(f(t))}{\partial t}=\frac{\partial C(f(t))}{\partial f(t)}\frac{\partial f(t)}{\partial t}\) The above is incorrect since I treated $f(t)$ like a scalar, when it is in fact, a function. If $f(t)$ is a vector, then we would sum the chain over its elements, i.e. $\sum_{i}\frac{\partial C(f(t))}{\partial f(t)_{i}}\frac{\partial f(t)_{i}}{\partial t}$. Here, we generalize this to the function $f(t)$ by treating a function as an infinite dimensional vector, which gives us an integral: \(\frac{\partial C(f(t))}{\partial t}=\int\frac{\partial C(f(t))}{\partial f(t)(x)}\frac{\partial f(t)(x)}{\partial t}dx\)</p>

<p>$\sum_{i}\frac{\partial C(f(t))}{\partial f(t)_{i}} \frac{\partial f(t)_{i}}{\partial t}$</p>

<p>So far, we ignored the data. When computing the cost, $C$ is not dependent on $f$ as a whole, but only dependent the outputs of $f$ where there are data points, e.g. $f(x_{i})$. Therefore, the above equation is equivalent to integrating over the data distribution $P(x)$, a sum of Dirac measures.</p>

<p>\(\frac{\partial C(f(t))}{\partial t}=\int\frac{\partial C(f(t))}{\partial f(t)(x)}\frac{\partial f(t)(x)}{\partial t}dP(x)\) There is one more correction to make on this derivative. $f(t)(x)$ is in $\mathbb{R}^{m}$, so with $i$ indexing the output channel, we finally have the correct form:</p>

<p>\(\frac{\partial C(f(t))}{\partial t}=\int\sum\_{i}^{m}\frac{\partial C(f(t))}{\partial f\_{i}(t)(x)}\frac{\partial f\_{i}(t)(x)}{\partial t}dP(x)\) Here we introduce a coordinate vector, i.e. an array of scalars, $d\vert_{f(t)}(x)$, whose $i^{th}$ element is $\frac{\partial C(f(t))}{\partial f_{i}(t)(x)}$. Also, we introduce a coordinate vector $\partial_{t}f(t)(x)$ whose element is $\frac{\partial f_{i}(t)(x)}{\partial t}$. From this construction, we see that $d\vert_{f(t)}$ and $\partial_{t}f(t)$ are functions ($d\vert_{f(t)}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}$; $\partial_{t}f(t):\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}$) in $\mathcal{F}$. So we can rewrite the derivative as:</p>

<p>\(\frac{\partial C(f(t))}{\partial t}=\mathbb{E}\_{x\sim P(x)}\left[d\vert\_{f(t)}(x)^{\top}\partial\_{t}f(t)(x)\right]\) Finally, we introduce a new notation \(\langle f,g\rangle\_{P(x)}=\mathbb{E}\_{x\sim P(x)}\left[f(x)^{\top}g(x)\right]\) which defines a seminorm $|\cdot|_{P(x)}$ in $\mathcal{F}$. Rewriting $\frac{\partial C(f(t))}{\partial t}$ as $\partial_{t}C\vert_{f(t)}$, the final form is</p>

<p>\(\partial\_{t}C\vert\_{f(t)}=\left\langle d\vert\_{f(t)},\partial\_{t}f(t)\right\rangle \_{P(x)}\) We haven’t yet discussed how exactly the cost value will evolve. We want it go down the steepest direction of the cost landscape. For that, we inspect the dynamics of $f(t)$ that guarantees the gradient descent.</p>

\[\theta(t+\eta)=\theta(t)-\eta\partial\_{\theta}(C\circ F)(\theta(t))\]

<p>\(\frac{\theta(t+\eta)-\theta(t)}{\eta}=-\partial\_{\theta}(C\circ F)(\theta(t))\) In the $\eta\rightarrow0$ limit, we have continuous dynamics for the each parameter $\theta$ (scalar).</p>

\[\partial\_{t}\theta(t)=-\partial\_{\theta}(C\circ F)(\theta(t))\]

\[\partial\_{\theta}(C\circ F)(\theta(t))=\frac{\partial C(F(\theta))}{\partial\theta}=\int\sum\_{i}\frac{\partial C(F(\theta))}{\partial F(\theta)\_{i}(x)}\frac{\partial F(\theta)\_{i}(x)}{\partial\theta}dP(x)\]

\[\frac{\partial C(f\_{\theta(t)})}{\partial\theta}=\int\sum\_{i}\frac{\partial C(f\_{\theta(t)})}{\partial f\_{\theta(t),i}(x)}\frac{\partial f\_{\theta(t),i}(x)}{\partial\theta}dP(x)\]

<p>\(\partial\_{\theta}C\vert\_{f\_{\theta(t)}}=\left\langle d\vert\_{f\_{\theta(t)}},\partial\_{\theta}f\_{\theta(t)}\right\rangle \_{P(x)}\) Therefore,</p>

<p>\(\partial\_{t}\theta(t)=-\left\langle d\vert\_{f\_{\theta(t)}},\partial\_{\theta}f\_{\theta(t)}\right\rangle \_{P(x)}\) How does $f$ evolve?</p>

\[\frac{\partial f\_{\theta(t)}}{\partial t}=\sum\_{j=1}^{N}\frac{\partial f\_{\theta(t)}}{\partial\theta\_{j}(t)}\frac{\partial\theta\_{j}(t)}{\partial t}\]

\[\partial\_{t}f\_{\theta(t)}=\sum\_{j=1}^{N}\partial\_{\theta\_{j}}f\_{\theta(t)}\partial\_{t}\theta\_{j}(t)\]

\[=-\sum\_{j=1}^{N}\partial\_{\theta\_{j}}f\_{\theta(t)}\left\langle d\vert\_{f\_{\theta(t)}},\partial\_{\theta\_{j}}f\_{\theta(t)}\right\rangle \_{P(x)}\]

<p>Assuming sample distribution (sum of deltas) for $P(x)$,</p>

\[=-\sum\_{j=1}^{N}\partial\_{\theta\_{j}}f\_{\theta(t)}\frac{1}{P}\sum\_{k=1}^{P}d\vert\_{f\_{\theta(t)}}(x\_{k})^{\top}\partial\_{\theta\_{j}}f\_{\theta(t)}(x\_{k})\]

<p>\(=-\frac{1}{P}\sum\_{k=1}^{P}\sum\_{j=1}^{N}\left(\partial\_{\theta\_{j}}f\_{\theta(t)}\right)\left[\partial\_{\theta\_{j}}f\_{\theta(t)}(x\_{k})\right]^{\top}\left[d\vert\_{f\_{\theta(t)}}(x\_{k})\right]\) Now re-express $\sum_{j=1}^{N}\left(\partial_{\theta_{j}}f_{\theta(t)}\right)\left[\partial_{\theta_{j}}f_{\theta(t)}(x_{k})\right]^{\top}$</p>

<p>\(\sum\_{j=1}^{N}\left(\partial\_{\theta\_{j}}f\_{\theta(t)}\right)\left[\partial\_{\theta\_{j}}f\_{\theta(t)}(x\_{k})\right]^{\top}=\sum\_{j=1}^{N}\left(\partial\_{\theta\_{j}}f\_{\theta(t)}\otimes\partial\_{\theta\_{j}}f\_{\theta(t)}\right)\left(\cdot,x\_{k}\right)\) with a different notation,</p>

\[=\sum\_{j=1}^{N}\left(\partial\_{\theta\_{j}}F(\theta)\otimes\partial\_{\theta\_{j}}F(\theta)\right)\left(\cdot,x\_{k}\right)\]

<p>\(=K\left(\cdot,x\_{k}\right)\) where $K$ is a tensor product, which is called tangent kernel:</p>

<p>\(K=\sum\_{j=1}^{N}\left(\partial\_{\theta\_{j}}F(\theta)\otimes\partial\_{\theta\_{j}}F(\theta)\right)\) Going back to the expression for the function dynamics, we have</p>

<p>\(\partial\_{t}f\_{\theta(t)}=-\frac{1}{P}\sum\_{k=1}^{P}K\left(\cdot,x\_{k}\right)d\vert\_{f\_{\theta(t)}}(x\_{k})\) In the paper, they call $\frac{1}{P}\sum_{k=1}^{P}K\left(\cdot,x_{k}\right)d\vert_{f_{\theta(t)}}(x_{k})$ a kernel gradient $\nabla_{K}C\in\mathcal{F}$</p>

<p>\(\nabla\_{K}C\vert\_{f\_{\theta(t)}}=\frac{1}{P}\sum\_{k=1}^{P}K\left(\cdot,x\_{k}\right)d\vert\_{f\_{\theta(t)}}(x\_{k})\) So they write the dynamics as,</p>

<p>\(\partial\_{t}f\_{\theta(t)}=-\nabla\_{K}C\vert\_{f\_{\theta(t)}}\) Now let’s revisit the dynamics of the cost value in $\mathbb{R}$.</p>

\[\partial\_{t}C\vert\_{f(t)}=\left\langle d\vert\_{f(t)},\partial\_{t}f(t)\right\rangle \_{P(x)}\]

\[=\frac{1}{P}\sum\_{i=1}^{P}d\vert\_{f(t)}(x\_{i})^{\top}\partial\_{t}f(t)(x\_{i})\]

\[=\frac{1}{P}\sum\_{i=1}^{P}d\vert\_{f(t)}(x\_{i})^{\top}\left[-\frac{1}{P}\sum\_{k=1}^{P}K\left(x\_{i},x\_{k}\right)d\vert\_{f(t)}(x\_{k})\right]\]

<p>\(=-\frac{1}{P^{2}}\sum\_{k=1}^{P}\sum\_{i=1}^{P}d\vert\_{f(t)}(x\_{i})^{\top}K\left(x\_{i},x\_{k}\right)d\vert\_{f(t)}(x\_{k})\) Here, we introduce another notation.</p>

<p>\(\langle f,g\rangle\_{K}=\mathbb{E}\_{x,x'\sim P(x)}\left[f(x)^{\top}K\left(x,x'\right)g(x')\right]\) and define $|f|_{K}^{2}=\langle f,f\rangle_{K}$. Remembering that we deal with a sample distribution,</p>

<p>\(\partial\_{t}C\vert\_{f(t)}=-\|d\vert\_{f(t)}\|\_{K}^{2}\) Also remember that $K$ is currently dependent on $\theta$, and since $\theta$ changes over time, $K$ is also dependent on time.</p>

<h1 id="neural-tangent-kernel">Neural Tangent Kernel</h1>

<p>All we need to do is compute $K=\sum_{j=1}^{N}\left(\partial_{\theta_{j}}F(\theta)\otimes\partial_{\theta_{j}}F(\theta)\right)$. More concretely,</p>

\[K(x,x')=\sum\_{j=1}^{N}\left[\partial\_{\theta\_{j}}f\_{\theta(t)}(x)\right]\left[\partial\_{\theta\_{j}}f\_{\theta(t)}(x')\right]^{\top}\]

<p>\(K\_{kk'}(x,x')=\sum\_{j=1}^{N}\left[\partial\_{\theta\_{j}}f\_{\theta(t)}(x)\right]\_{k}\left[\partial\_{\theta\_{j}}f\_{\theta(t)}(x')\right]\_{k'}\) where $\left[\cdot\right]_{k}$ denotes the $k^{th}$ channel of the output. Start with the simplest model $L=1$ (input layer and output layer, no nonlinearity).</p>

<p>\(f\_{\theta}(x)=\frac{1}{\sqrt{n\_{0}}}W^{(0)}x+\beta b^{(0)}\) Now realize what the summand of $K_{kk’}^{(1)}(x,x’)$ should be for this model.</p>

<p>\(K\_{kk'}^{(1)}(x,x')=\sum\_{i=1}^{n\_{0}}\sum\_{j=1}^{n\_{1}}\left[\partial\_{W\_{ji}^{(0)}}f\_{\theta}(x)\right]\_{k}\left[\partial\_{W\_{ji}^{(0)}}f\_{\theta}(x')\right]\_{k'}+\sum\_{j=1}^{n\_{1}}\left[\partial\_{b\_{j}^{(0)}}f\_{\theta}(x)\right]\_{k}\left[\partial\_{b\_{j}^{(0)}}f\_{\theta}(x')\right]\_{k'}\) Note that $\left[\partial_{W_{ji}^{(0)}}f_{\theta}(x)\right]_{k}$ is $0$ if $j\neq k$, i.e. there is no change in the output of the $k^{th}$ channel of $f_{\theta}(x)$ when there is a change in $W_{ji}$ for $j\neq k$. Therefore,</p>

\[K\_{kk'}^{(1)}(x,x')=\sum\_{i=1}^{n\_{0}}\sum\_{j=1}^{n\_{1}}\left(\partial\_{W\_{ji}^{(0)}}f\_{\theta}(x)\right)\left(\partial\_{W\_{ji}^{(0)}}f\_{\theta}(x')\right)\delta\_{jk}\delta\_{jk'}+\sum\_{j=1}^{n\_{1}}\left(\partial\_{b\_{j}^{(0)}}f\_{\theta}(x)\right)\left(\partial\_{b\_{j}^{(0)}}f\_{\theta}(x')\right)\delta\_{jk}\delta\_{jk'}\]

<p>Since</p>

\[\partial\_{W\_{ji}^{(0)}}f\_{\theta}(x)=\frac{1}{\sqrt{n\_{0}}}x\_{i}\]

<p>\(\partial\_{b\_{j}^{(0)}}f\_{\theta}(x)=\beta\) , the kernel is</p>

\[K\_{kk'}^{(1)}(x,x')=\frac{1}{n\_{0}}\sum\_{i=1}^{n\_{0}}\sum\_{j=1}^{n\_{1}}x\_{i}x'\_{i}\delta\_{jk}\delta\_{jk'}+\beta^{2}\sum\_{j=1}^{n\_{1}}\delta\_{jk}\delta\_{jk'}\]

\[K\_{kk'}^{(1)}(x,x')=\frac{1}{n\_{0}}x^{\top}x'\delta\_{kk'}+\beta^{2}\delta\_{kk'}\]

\[=\left(\frac{1}{n\_{0}}x^{\top}x'+\beta^{2}\right)\delta\_{kk'}\]

<p>\(=\Sigma^{(1)}(x,x')\delta\_{kk'}\) . Next, we generalize this to deeper layers. We will see how $K^{(L+1)}(x,x’)$ is a function of $K^{(L)}(x,x’)$, and this relationship allows us to compute a tangent kernel for an arbitrary deep network.</p>

<p>\(K\_{kk'}^{(L+1)}(x,x')=\sum\_{j=1}^{\widetilde{N}}\left[\partial\_{\widetilde{\theta}\_{j}}f\_{\theta}(x)\right]\_{k}\left[\partial\_{\widetilde{\theta}\_{j}}f\_{\theta}(x')\right]\_{k'}+\sum\_{l=1}^{\hat{N}}\left[\partial\_{\hat{\theta}\_{l}}f\_{\theta}(x)\right]\_{k}\left[\partial\_{\hat{\theta}\_{l}}f\_{\theta}(x')\right]\_{k'}\) For the similar reason as in $L=1$, $\sum_{l=1}^{\hat{N}}\left[\partial_{\hat{\theta}_{l}}f_{\theta}(x)\right]_{k}\left[\partial_{\hat{\theta}_{l}}f_{\theta}(x’)\right]_{k’}=\Sigma^{(L+1)}(x,x’)\delta_{kk’}$. Let’s dissect $\partial_{\widetilde{\theta}_{j}}f_{\theta}(x)$, where $f_{\theta}^{(L+1)}(x)=\frac{1}{\sqrt{n_{L}}}W^{(L)}\alpha_{\widetilde{\theta}}^{(L)}(x)+\beta b^{(L)}$</p>

\[\partial\_{\widetilde{\theta}\_{j}}f\_{\theta}(x)=\partial\_{\widetilde{\theta}\_{j}}\left(\frac{1}{\sqrt{n\_{L}}}W^{(L)}\alpha\_{\widetilde{\theta}}^{(L)}(x)+\beta b^{(L)}\right)\]

<p>\(=\frac{1}{\sqrt{n\_{L}}}W^{(L)}\left(\partial\_{\widetilde{\theta}\_{j}}\alpha\_{\widetilde{\theta}}^{(L)}(x)\right)\) $\partial_{\widetilde{\theta}_{j}}\alpha^{(L)}(x)\in\mathbb{R}^{n_{L}}$.</p>

<p>\(\left[\partial\_{\widetilde{\theta}\_{j}}f\_{\theta}(x)\right]\_{k}=\frac{1}{\sqrt{n\_{L}}}\sum\_{i=1}^{n\_{L}}W\_{ki}^{(L)}\left[\partial\_{\widetilde{\theta}\_{j}}\alpha\_{\widetilde{\theta}}^{(L)}(x)\right]\_{i}\) Note that $\left[\partial_{\widetilde{\theta}_{j}}\alpha^{(L)}(x)\right]_{i}=\left[\partial_{\widetilde{\theta}_{j}}\tilde{\alpha}^{(L)}(x)\right]_{i}\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}^{(L)}(x)\right)}}\right]_{i}$.</p>

<p>\(\left[\partial\_{\widetilde{\theta}\_{j}}f\_{\theta}(x)\right]\_{k}=\sum\_{i=1}^{n\_{L}}\frac{1}{\sqrt{n\_{L}}}W\_{ki}^{(L)}\left[\partial\_{\widetilde{\theta}\_{j}}\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x)\right]\_{i}\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x)\right)}}\right]\_{i}\) So the first summation is</p>

<p>\(\sum\_{j=1}^{\widetilde{N}}\left[\partial\_{\widetilde{\theta}\_{j}}f\_{\theta}(x)\right]\_{k}\left[\partial\_{\widetilde{\theta}\_{j}}f\_{\theta}(x')\right]\_{k'}=\frac{1}{n\_{L}}\sum\_{i,i'=1}^{n\_{L}}\left\{ \sum\_{j=1}^{\widetilde{N}}\left[\partial\_{\widetilde{\theta}\_{j}}\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x)\right]\_{i}\left[\partial\_{\widetilde{\theta}\_{j}}\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x')\right]\_{i'}\right\} \left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x)\right)}}\right]\_{i}\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x')\right)}}\right]\_{i'}W\_{ki}^{(L)}W\_{k'i'}^{(L)}\) Note that $\sum_{j=1}^{\widetilde{N}}\left[\partial_{\widetilde{\theta}_{j}}\tilde{\alpha}^{(L)}(x)\right]_{i}\left[\partial_{\widetilde{\theta}_{j}}\tilde{\alpha}^{(L)}(x)\right]_{i’}=K_{ii’}^{(L)}(x,x’)$.</p>

<p>\(\sum\_{j=1}^{\widetilde{N}}\left[\partial\_{\widetilde{\theta}\_{j}}f\_{\theta}(x)\right]\_{k}\left[\partial\_{\widetilde{\theta}\_{j}}f\_{\theta}(x')\right]\_{k'}=\frac{1}{n\_{L}}\sum\_{i,i'=1}^{n\_{L}}\left(K\_{ii'}^{(L)}(x,x')\right)\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x)\right)}}\right]\_{i}\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x')\right)}}\right]\_{i'}W\_{ki}^{(L)}W\_{k'i'}^{(L)}\) Therefore,</p>

<p>\(K\_{kk'}^{(L+1)}(x,x')=\frac{1}{n\_{L}}\sum\_{i,i'=1}^{n\_{L}}\left(K\_{ii'}^{(L)}(x,x')\right)\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x)\right)}}\right]\_{i}\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x')\right)}}\right]\_{i'}W\_{ki}^{(L)}W\_{k'i'}^{(L)}+\Sigma^{(L+1)}(x,x')\delta\_{kk'}\) To simplify further, we need to take $n_{L}\rightarrow\infty$. First, $K_{ii’}^{(L)}(x,x’)$ simplifies (Proposition 1). \(K\_{ii'}^{(L)}(x,x')\xrightarrow{n\_{l}\rightarrow\infty\forall l}K\_{\infty}^{(L)}(x,x')\delta\_{ii'}\) Detailed proof is required to show we need to sequentially expand the widths from the first hidden layer to the next and so on. We prove Proposition 1 by showing that the summation term in the above $K_{kk’}^{(L+1)}(x,x’)$ equation has the $\delta_{kk’}$ factor when $n_{l}\rightarrow\infty\quad\forall l\leq L$. We don’t need to worry about the second term, since it already has $\delta_{kk’}$. When $n_{l}\rightarrow\infty\quad\forall l\leq L$, the summation becomes expectation over the randomness in the network induced by the random weights and biases.</p>

\[\sum\_{j=1}^{\widetilde{N}}\left[\partial\_{\widetilde{\theta}\_{j}}f\_{\theta}(x)\right]\_{k}\left[\partial\_{\widetilde{\theta}\_{j}}f\_{\theta}(x')\right]\_{k'}\xrightarrow{n\_{l}\rightarrow\infty\forall l}\mathbb{E}\_{\widetilde{\theta}\sim P(\theta)}\left[\left(K\_{ii'}^{(L)}(x,x')\right)\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x)\right)}}\right]\_{i}\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x')\right)}}\right]\_{i'}W\_{ki}^{(L)}W\_{k'i'}^{(L)}\right]\]

<p>\(=\mathbb{E}\_{\widetilde{\theta}\sim P(\theta)}\left[\left(K\_{ii'}^{(L)}(x,x')\right)\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x)\right)}}\right]\_{i}\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x')\right)}}\right]\_{i'}\right]\mathbb{E}\_{\widetilde{\theta}\sim P(\theta)}\left[W\_{ki}^{(L)}W\_{k'i'}^{(L)}\right]\) The decomposition of the expectation in the second equality is due to the independence between the randomness of $W^{(L)}$ and the randomness of $\tilde{\alpha}_{\widetilde{\theta}}^{(L)}(x)$. It is evident that $\mathbb{E}_{\widetilde{\theta}\sim P(\theta)}\left[W_{ki}^{(L)}W_{k’i’}^{(L)}\right]=\delta_{kk’}\delta_{ii’}$. Therefore, at the large width limit,</p>

<p>\(K\_{kk'}^{(L+1)}(x,x')=\frac{1}{n\_{L}}\sum\_{i=1}^{n\_{L}}\left(K\_{ii}^{(L)}(x,x')\right)\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x)\right)}}\right]\_{i}\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x')\right)}}\right]\_{i}\delta\_{kk'}+\Sigma^{(L+1)}(x,x')\delta\_{kk'}\) Due to symmetry of all neurons at the large width limit, the summation becomes an average over $i$’s.</p>

<p>\(K\_{kk'}^{(L+1)}(x,x')=\mathbb{E}\_{\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}\sim\mathcal{GP}}\left[\left(K\_{ii}^{(L)}(x,x')\right)\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x)\right)}}\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x')\right)}}\right]\delta\_{kk'}+\Sigma^{(L+1)}(x,x')\delta\_{kk'}\) This shows that $K_{kk’}^{(L+1)}(x,x’)$ concentrates as $n_{L}\rightarrow\infty$. Here it is important to note that we take $n_{l}\rightarrow\infty\quad\forall l\leq L-1$ before $n_{L}\rightarrow\infty$, i.e. before the above averaging $\mathbb{E}_{i}[\cdot]$ appears. That is because, by taking $n_{L-1}\rightarrow\infty$ first, we concentrate $K_{ii}^{(L)}(x,x’)$ to $K_{\infty}^{(L)}(x,x’)$, such that we have $K_{\infty}^{(L)}(x,x’)$ outside the above $\mathbb{E}_{i}[\cdot]$ when we take $n_{L}\rightarrow\infty$. Following through with this order of expansion, we have</p>

\[K\_{kk'}^{(L+1)}(x,x')=K\_{\infty}^{(L)}(x,x')\mathbb{E}\_{i}\left[\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x)\right)}}\right]\_{i}\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}\_{\widetilde{\theta}}^{(L)}(x')\right)}}\right]\_{i}\right]\delta\_{kk'}+\Sigma^{(L+1)}(x,x')\delta\_{kk'}\]

<p>Introducing new notation $\dot{\Sigma}^{(L+1)}=\mathbb{E}_{\tilde{\alpha}_{\widetilde{\theta}}^{(L)}\sim\mathcal{GP}}\left[\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}_{\widetilde{\theta}}^{(L)}(x)\right)}}\dot{\sigma}\text{\ensuremath{\left(\tilde{\alpha}_{\widetilde{\theta}}^{(L)}(x’)\right)}}\right]$ , we have</p>

\[K\_{kk'}^{(L+1)}(x,x')=\left(K\_{\infty}^{(L)}(x,x')\dot{\Sigma}^{(L+1)}+\Sigma^{(L+1)}(x,x')\right)\delta\_{kk'}\]

<p>\(K\_{\infty}^{(L+1)}(x,x')=K\_{\infty}^{(L)}(x,x')\dot{\Sigma}^{(L+1)}+\Sigma^{(L+1)}(x,x')\) To summarize,</p>

\[K\_{\infty}^{(1)}(x,x')=\Sigma^{(1)}(x,x')\]

\[K\_{\infty}^{(L+1)}(x,x')=K\_{\infty}^{(L)}(x,x')\dot{\Sigma}^{(L+1)}+\Sigma^{(L+1)}(x,x')\]]]></content><author><name>Chanwoo Chun</name><email>cc2465@cornell.edu</email></author><summary type="html"><![CDATA[Despite the popularity and importance of Neural Tangent Kernel, the original paper by Jacot et al. is not an easy read. Thankfully, there are a lot of lecture notes and blog posts available online that explains NTK in very simple terms, but none (to the best of my knowledge) illustrates the generality of “tangent kernel” that explains the evolution of any parameteric model descending down on any kind of cost function. This is in part because “tangent kernel” part of the paper is a bit abstract. Moreover, the general public (myself included) is more interested in what the paper has to say about neural networks and least-squares cost, where the derivation is less abstract and easier to follow.]]></summary></entry><entry><title type="html">Notes on Temporal Difference</title><link href="http://localhost:4000/study/note/2023/03/31/temporal-difference.html" rel="alternate" type="text/html" title="Notes on Temporal Difference" /><published>2023-03-31T00:00:00-04:00</published><updated>2023-03-31T00:00:00-04:00</updated><id>http://localhost:4000/study/note/2023/03/31/temporal-difference</id><content type="html" xml:base="http://localhost:4000/study/note/2023/03/31/temporal-difference.html"><![CDATA[<p>This is my note on temporal difference. Heavily based on <a href="http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/kirsty/Learning.pdf">(link)</a>.</p>

<h3 id="rescorla-wagner-rule">Rescorla-Wagner rule</h3>

<p>Consider a model that outputs expected reward ($v$), given some stimulus ($\mathbf{x}$).</p>
<div>
\begin{equation}
 v= \mathbf{w}^{\top}\mathbf{x}
\end{equation}
</div>
<p>The real reward that the model receive, $v_d$, is our “data”. This model does not have a temporal componenet. We update the weights $\mathbf{w}$ of the model by minimizing the square error of the prediction and the data.</p>
<div>
\begin{align}
\min_\mathbf{w} &amp; E(\mathbf{w}) \\
 E(\mathbf{w}) &amp;= \frac{1}{2}(v_d- \mathbf{w}^\top \mathbf{x})^2
\end{align}
</div>
<p>We perform gradient descent to find the optimal $\mathbf{w}$</p>
<div>
\begin{equation}
\mathbf{w} \leftarrow \mathbf{w} - \epsilon \frac{dE}{d\mathbf{w}}
\end{equation}

\begin{equation}
\frac{dE}{d\mathbf{w}} = -(v_d-\mathbf{w}^\top \mathbf{x})\mathbf{x}
\end{equation}
</div>

<p>Therefore, the final form of the update rule is usually written as</p>
<div>
\begin{equation}
\mathbf{w} \leftarrow \mathbf{w} + \epsilon \delta \mathbf{x}
\end{equation}
\begin{equation}
\delta = v_d-v
\end{equation}
</div>
<p>$\delta$ is known as “prediction error”, “which has biological analogues in the activity of dopaminergic cells in the VTA.”</p>

<h3 id="temporal-difference-learning">Temporal difference learning</h3>

<p>Now we consider the temporal component to our scenario. In this case, the definition of “expected reward” ($v(t)$) is expected value of the sum of future rewards at time $t$.</p>

<div>
\begin{equation}
v(t)=\langle \sum_{\tau=0}^{T-t}  r(t+\tau) \rangle
\end{equation}
</div>

<p>Then what is $v_d(t)$? $v_d(t)$ would be the true expected reward at time $t$. However, now the problem is at time $t$, it is impossible to know what the true future reward will be. Hence it is impossible compute $\delta$. Fortunately, there is a trick that allows us to converge to the true $v_d(t)$ as we make observations without messing with the arrow of time.</p>

<div>
\begin{align}
v(t)&amp;=\langle \sum_{\tau=0}^{T-t}  r(t+\tau) \rangle \\
&amp;=\langle r_t \rangle + v(t+1)
\end{align}
\begin{equation}
\langle r_t \rangle = v(t)-v(t+1)
\end{equation}
</div>

<p>$r_d(t) \equiv \langle r_t \rangle$ is the actual reward that we receive in one time step. The prediction error is now</p>

<div>
\begin{align}
\delta(t) &amp;= r_d(t) + v(t+1)-v(t)
\end{align}
</div>]]></content><author><name>Chanwoo Chun</name><email>cc2465@cornell.edu</email></author><category term="study" /><category term="note" /><summary type="html"><![CDATA[This is my note on temporal difference. Heavily based on (link).]]></summary></entry></feed>